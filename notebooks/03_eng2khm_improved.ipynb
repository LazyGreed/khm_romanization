{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d994e6e0",
   "metadata": {},
   "source": [
    "# Improved English to Khmer Transliteration with Advanced Techniques\n",
    "\n",
    "This notebook implements several improvements over the baseline seq2seq model:\n",
    "- **Attention Mechanism**: Helps decoder focus on relevant input parts\n",
    "- **Bidirectional Encoder**: Captures context from both directions\n",
    "- **Deeper Networks**: Multi-layer LSTMs for increased capacity\n",
    "- **Beam Search**: Better quality decoding with beam width of 3\n",
    "- **Data Augmentation**: Increases training data diversity\n",
    "- **Cross-Validation**: K-fold validation for robust evaluation\n",
    "- **Comprehensive Metrics**: BLEU, Character Error Rate (CER), Word Error Rate (WER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fde34ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPU Available: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Attention, Concatenate\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Configuration parameters\n",
    "EMBED_DIM = 64  # Increased from 32\n",
    "LSTM_UNITS = 128  # Increased from 64\n",
    "NUM_LAYERS = 2  # Multi-layer LSTMs\n",
    "BATCH_SIZE = 32  # Increased batch size\n",
    "EPOCHS = 100\n",
    "BEAM_WIDTH = 3  # For beam search decoding\n",
    "K_FOLDS = 5  # For cross-validation\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('.'))\n",
    "DATA_PATH = os.path.join(BASE_DIR, \"data\", \"raw\", \"eng_khm_data.csv\")\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"models\", \"english_romanizer.keras\")\n",
    "ASSETS_PATH = os.path.join(BASE_DIR, \"data\", \"processed\", \"khmer_improved_assets.pkl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d140898",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b61fd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 28576\n",
      "Dataset size after augmentation: 37024\n",
      "Sample pairs:\n",
      "  brodae -> ប្រដែ\n",
      "  aasangkheyy -> អសង្ខៃយ\n",
      "  chhatkophey -> ឆាតកភ័យ\n",
      "  topvosompheareak -> ទព្វសម្ភារៈ\n",
      "  topvosompheareak -> ទព្វសម្ភារៈ\n"
     ]
    }
   ],
   "source": [
    "def augment_data(eng, khm):\n",
    "    \"\"\"Apply data augmentation techniques to increase diversity\"\"\"\n",
    "    augmented = [(eng, khm)]  # Original pair\n",
    "    \n",
    "    # Augmentation 1: Add random noise (character substitution with low probability)\n",
    "    if len(eng) > 2 and np.random.random() < 0.3:\n",
    "        chars = list(eng)\n",
    "        idx = np.random.randint(0, len(chars))\n",
    "        # Small chance to duplicate a character\n",
    "        if np.random.random() < 0.5:\n",
    "            chars.insert(idx, chars[idx])\n",
    "        augmented.append((''.join(chars), khm))\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "# Load and preprocess dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "\n",
    "dataset = []\n",
    "for _, row in df.iterrows():\n",
    "    # Normalize English text\n",
    "    normalized_eng = re.sub(r\"[^a-z]\", \"\", row['eng'].lower())\n",
    "    \n",
    "    # Normalize Khmer text\n",
    "    normalized_khm = re.sub(r\"[^\\u1780-\\u17FF]\", \"\", row['khm'])\n",
    "    normalized_khm = unicodedata.normalize('NFC', normalized_khm)\n",
    "    \n",
    "    if normalized_eng and normalized_khm:  # Only add non-empty pairs\n",
    "        # Add original and augmented versions\n",
    "        augmented_pairs = augment_data(normalized_eng, normalized_khm)\n",
    "        dataset.extend(augmented_pairs)\n",
    "\n",
    "print(f\"Dataset size after augmentation: {len(dataset)}\")\n",
    "print(f\"Sample pairs:\")\n",
    "for i in range(5):\n",
    "    print(f\"  {dataset[i][0]} -> {dataset[i][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "786b8d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 28\n",
      "Khmer vocab size: 81\n",
      "Max English length: 25\n",
      "Max Khmer length: 24\n"
     ]
    }
   ],
   "source": [
    "# Tokenize English and Khmer texts\n",
    "eng_tokenizer = Tokenizer(char_level=True, filters='', oov_token='<unk>')\n",
    "eng_tokenizer.fit_on_texts([pair[0] for pair in dataset])\n",
    "\n",
    "khm_tokenizer = Tokenizer(char_level=True, filters='', oov_token='<unk>')\n",
    "khm_tokenizer.fit_on_texts([\"\\t\", \"\\n\"] + [pair[1] for pair in dataset])\n",
    "\n",
    "print(f\"English vocab size: {len(eng_tokenizer.word_index) + 1}\")\n",
    "print(f\"Khmer vocab size: {len(khm_tokenizer.word_index) + 1}\")\n",
    "\n",
    "# Calculate max lengths\n",
    "max_eng_len = max(len(pair[0]) for pair in dataset)\n",
    "max_khm_len = max(len(pair[1]) for pair in dataset)\n",
    "print(f\"Max English length: {max_eng_len}\")\n",
    "print(f\"Max Khmer length: {max_khm_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3597938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder data shape: (37024, 25)\n",
      "Decoder input shape: (37024, 25)\n",
      "Decoder target shape: (37024, 25)\n"
     ]
    }
   ],
   "source": [
    "# Create sequences for training\n",
    "def prepare_sequences(dataset):\n",
    "    encoder_inputs, decoder_inputs, decoder_targets = [], [], []\n",
    "    \n",
    "    for eng, khm in dataset:\n",
    "        # Encoder sequence\n",
    "        eng_seq = eng_tokenizer.texts_to_sequences([eng])[0]\n",
    "        encoder_inputs.append(eng_seq)\n",
    "        \n",
    "        # Decoder sequences\n",
    "        khm_seq = khm_tokenizer.texts_to_sequences([khm])[0]\n",
    "        decoder_input = [khm_tokenizer.word_index['\\t']] + khm_seq\n",
    "        decoder_target = khm_seq + [khm_tokenizer.word_index['\\n']]\n",
    "        \n",
    "        decoder_inputs.append(decoder_input)\n",
    "        decoder_targets.append(decoder_target)\n",
    "    \n",
    "    encoder_data = pad_sequences(encoder_inputs, maxlen=max_eng_len, padding='post')\n",
    "    decoder_input_data = pad_sequences(decoder_inputs, maxlen=max_khm_len + 1, padding='post')\n",
    "    decoder_target_data = pad_sequences(decoder_targets, maxlen=max_khm_len + 1, padding='post')\n",
    "    \n",
    "    return encoder_data, decoder_input_data, decoder_target_data\n",
    "\n",
    "encoder_data, decoder_input_data, decoder_target_data = prepare_sequences(dataset)\n",
    "print(f\"Encoder data shape: {encoder_data.shape}\")\n",
    "print(f\"Decoder input shape: {decoder_input_data.shape}\")\n",
    "print(f\"Decoder target shape: {decoder_target_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d305767d",
   "metadata": {},
   "source": [
    "## Improved Model Architecture\n",
    "\n",
    "Key improvements:\n",
    "- **Bidirectional LSTM Encoder**: Captures context from both directions\n",
    "- **Multi-layer LSTMs**: 2 layers for both encoder and decoder\n",
    "- **Attention Mechanism**: Allows decoder to focus on relevant encoder outputs\n",
    "- **Increased Capacity**: Larger embedding and hidden dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e6e18c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"improved_seq2seq\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"improved_seq2seq\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_bilstm_1    │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">197,632</span> │ encoder_embeddin… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_bilstm_2    │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ encoder_bilstm_1… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,184</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_bilstm_2… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ encoder_bilstm_2… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_bilstm_2… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ encoder_bilstm_2… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm_1      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">328,704</span> │ decoder_embeddin… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm_2      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ decoder_lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_layer     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │ encoder_bilstm_2… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concat_layer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ attention_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">41,553</span> │ concat_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │      \u001b[38;5;34m1,792\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_bilstm_1    │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m197,632\u001b[0m │ encoder_embeddin… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_bilstm_2    │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m394,240\u001b[0m │ encoder_bilstm_1… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │      \u001b[38;5;34m5,184\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ encoder_bilstm_2… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ encoder_bilstm_2… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ encoder_bilstm_2… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ encoder_bilstm_2… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm_1      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m328,704\u001b[0m │ decoder_embeddin… │\n",
       "│ (\u001b[38;5;33mLSTM\u001b[0m)              │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm_2      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m525,312\u001b[0m │ decoder_lstm_1[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mLSTM\u001b[0m)              │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_layer     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ decoder_lstm_2[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │ encoder_bilstm_2… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concat_layer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ decoder_lstm_2[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ attention_layer[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m81\u001b[0m)  │     \u001b[38;5;34m41,553\u001b[0m │ concat_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,494,417</span> (5.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,494,417\u001b[0m (5.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,494,417</span> (5.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,494,417\u001b[0m (5.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_improved_model():\n",
    "    \"\"\"Build seq2seq model with attention, bidirectional encoder, and deeper networks\"\"\"\n",
    "    \n",
    "    # Encoder with Bidirectional LSTM\n",
    "    encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
    "    encoder_embedding = Embedding(\n",
    "        input_dim=len(eng_tokenizer.word_index) + 1,\n",
    "        output_dim=EMBED_DIM,\n",
    "        mask_zero=True,\n",
    "        name='encoder_embedding'\n",
    "    )(encoder_inputs)\n",
    "    \n",
    "    # First bidirectional layer\n",
    "    encoder_lstm1 = Bidirectional(\n",
    "        LSTM(LSTM_UNITS, return_sequences=True, return_state=True),\n",
    "        name='encoder_bilstm_1'\n",
    "    )\n",
    "    encoder_outputs1, forward_h1, forward_c1, backward_h1, backward_c1 = encoder_lstm1(encoder_embedding)\n",
    "    \n",
    "    # Second bidirectional layer\n",
    "    encoder_lstm2 = Bidirectional(\n",
    "        LSTM(LSTM_UNITS, return_sequences=True, return_state=True),\n",
    "        name='encoder_bilstm_2'\n",
    "    )\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm2(encoder_outputs1)\n",
    "    \n",
    "    # Combine forward and backward states\n",
    "    state_h = Concatenate()([forward_h, backward_h])\n",
    "    state_c = Concatenate()([forward_c, backward_c])\n",
    "    \n",
    "    # Decoder with Attention\n",
    "    decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
    "    decoder_embedding = Embedding(\n",
    "        input_dim=len(khm_tokenizer.word_index) + 1,\n",
    "        output_dim=EMBED_DIM,\n",
    "        mask_zero=True,\n",
    "        name='decoder_embedding'\n",
    "    )(decoder_inputs)\n",
    "    \n",
    "    # First decoder LSTM layer\n",
    "    decoder_lstm1 = LSTM(LSTM_UNITS * 2, return_sequences=True, return_state=True, name='decoder_lstm_1')\n",
    "    decoder_outputs1, _, _ = decoder_lstm1(decoder_embedding, initial_state=[state_h, state_c])\n",
    "    \n",
    "    # Second decoder LSTM layer\n",
    "    decoder_lstm2 = LSTM(LSTM_UNITS * 2, return_sequences=True, return_state=True, name='decoder_lstm_2')\n",
    "    decoder_outputs, _, _ = decoder_lstm2(decoder_outputs1)\n",
    "    \n",
    "    # Attention layer\n",
    "    attention = Attention(name='attention_layer')\n",
    "    context_vector = attention([decoder_outputs, encoder_outputs])\n",
    "    \n",
    "    # Concatenate attention output with decoder output\n",
    "    decoder_combined = Concatenate(name='concat_layer')([decoder_outputs, context_vector])\n",
    "    \n",
    "    # Output layer\n",
    "    decoder_dense = Dense(\n",
    "        len(khm_tokenizer.word_index) + 1,\n",
    "        activation='softmax',\n",
    "        name='decoder_dense'\n",
    "    )\n",
    "    decoder_outputs_final = decoder_dense(decoder_combined)\n",
    "    \n",
    "    # Build and compile model\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs_final, name='improved_seq2seq')\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_improved_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae323c4",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation Training\n",
    "\n",
    "Using 5-fold cross-validation for more robust performance estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f3e77a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold cross-validation...\n",
      "Total samples: 37024\n",
      "Epochs per fold: 100\n",
      "Batch size: 32\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Prepare for k-fold cross-validation\n",
    "kfold = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "fold_histories = []\n",
    "fold_scores = []\n",
    "\n",
    "# Callbacks for training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"Starting {K_FOLDS}-fold cross-validation...\")\n",
    "print(f\"Total samples: {len(encoder_data)}\")\n",
    "print(f\"Epochs per fold: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21d2c2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training Fold 1/5\n",
      "======================================================================\n",
      "Training samples: 29619\n",
      "Validation samples: 7405\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 17:04:27.988915: E tensorflow/core/util/util.cc:131] oneDNN supports DT_BOOL only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m459/926\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 104ms/step - accuracy: 0.2555 - loss: 3.1080"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m fold_model = build_improved_model()\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m history = \u001b[43mfold_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoder_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_target_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoder_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_target_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     33\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n\u001b[32m     36\u001b[39m val_loss, val_accuracy = fold_model.evaluate(\n\u001b[32m     37\u001b[39m     [encoder_val, decoder_input_val],\n\u001b[32m     38\u001b[39m     np.expand_dims(decoder_target_val, -\u001b[32m1\u001b[39m),\n\u001b[32m     39\u001b[39m     verbose=\u001b[32m0\u001b[39m\n\u001b[32m     40\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/khmer_romanization/.venv/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/khmer_romanization/.venv/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/khmer_romanization/.venv/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/khmer_romanization/.venv/lib/python3.13/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/khmer_romanization/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/khmer_romanization/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/khmer_romanization/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/khmer_romanization/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/khmer_romanization/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/khmer_romanization/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/khmer_romanization/.venv/lib/python3.13/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/khmer_romanization/.venv/lib/python3.13/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# K-fold cross-validation training loop\n",
    "best_fold_idx = -1\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(encoder_data)):\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Training Fold {fold_idx + 1}/{K_FOLDS}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    \n",
    "    # Split data\n",
    "    encoder_train, encoder_val = encoder_data[train_idx], encoder_data[val_idx]\n",
    "    decoder_input_train, decoder_input_val = decoder_input_data[train_idx], decoder_input_data[val_idx]\n",
    "    decoder_target_train, decoder_target_val = decoder_target_data[train_idx], decoder_target_data[val_idx]\n",
    "    \n",
    "    print(f\"Training samples: {len(encoder_train)}\")\n",
    "    print(f\"Validation samples: {len(encoder_val)}\")\n",
    "    \n",
    "    # Build fresh model for this fold\n",
    "    fold_model = build_improved_model()\n",
    "    \n",
    "    # Train model\n",
    "    history = fold_model.fit(\n",
    "        [encoder_train, decoder_input_train],\n",
    "        np.expand_dims(decoder_target_train, -1),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(\n",
    "            [encoder_val, decoder_input_val],\n",
    "            np.expand_dims(decoder_target_val, -1)\n",
    "        ),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_accuracy = fold_model.evaluate(\n",
    "        [encoder_val, decoder_input_val],\n",
    "        np.expand_dims(decoder_target_val, -1),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx + 1} Results:\")\n",
    "    print(f\"  Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    fold_histories.append(history)\n",
    "    fold_scores.append({'loss': val_loss, 'accuracy': val_accuracy})\n",
    "    \n",
    "    # Keep track of best fold\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_fold_idx = fold_idx\n",
    "        # Save best model\n",
    "        fold_model.save(MODEL_PATH)\n",
    "        print(f\"  ✓ Best model so far! Saved to {MODEL_PATH}\")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"Cross-Validation Complete!\")\n",
    "print(f\"Best fold: {best_fold_idx + 1} with validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"{'=' * 70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90631245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display cross-validation summary statistics\n",
    "avg_val_loss = np.mean([score['loss'] for score in fold_scores])\n",
    "std_val_loss = np.std([score['loss'] for score in fold_scores])\n",
    "avg_val_accuracy = np.mean([score['accuracy'] for score in fold_scores])\n",
    "std_val_accuracy = np.std([score['accuracy'] for score in fold_scores])\n",
    "\n",
    "print(\"\\nCross-Validation Summary:\")\n",
    "print(f\"  Average Validation Loss: {avg_val_loss:.4f} ± {std_val_loss:.4f}\")\n",
    "print(f\"  Average Validation Accuracy: {avg_val_accuracy:.4f} ± {std_val_accuracy:.4f}\")\n",
    "print(\"\\nPer-Fold Results:\")\n",
    "for i, score in enumerate(fold_scores):\n",
    "    print(f\"  Fold {i+1}: Loss={score['loss']:.4f}, Accuracy={score['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2885f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizers and metadata\n",
    "assets = {\n",
    "    \"eng_tokenizer\": eng_tokenizer,\n",
    "    \"khm_tokenizer\": khm_tokenizer,\n",
    "    \"max_eng_len\": max_eng_len,\n",
    "    \"max_khm_len\": max_khm_len,\n",
    "    \"fold_scores\": fold_scores,\n",
    "    \"avg_val_loss\": avg_val_loss,\n",
    "    \"avg_val_accuracy\": avg_val_accuracy\n",
    "}\n",
    "\n",
    "with open(ASSETS_PATH, \"wb\") as file:\n",
    "    pickle.dump(assets, file)\n",
    "\n",
    "print(f\"Assets saved to {ASSETS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46cafe5",
   "metadata": {},
   "source": [
    "## Inference with Beam Search Decoding\n",
    "\n",
    "Implementing beam search for better quality predictions by exploring multiple candidate sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model and assets\n",
    "model = load_model(MODEL_PATH)\n",
    "print(f\"Loaded best model from {MODEL_PATH}\")\n",
    "\n",
    "with open(ASSETS_PATH, \"rb\") as file:\n",
    "    assets = pickle.load(file)\n",
    "\n",
    "eng_tokenizer = assets[\"eng_tokenizer\"]\n",
    "khm_tokenizer = assets[\"khm_tokenizer\"]\n",
    "max_eng_len = assets[\"max_eng_len\"]\n",
    "max_khm_len = assets[\"max_khm_len\"]\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Average CV accuracy: {assets['avg_val_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build encoder model for inference\n",
    "encoder_inputs = model.input[0]\n",
    "encoder_outputs, forward_h1, forward_c1, backward_h1, backward_c1 = model.get_layer('encoder_bilstm_1').output\n",
    "\n",
    "# Get outputs from second bidirectional layer\n",
    "encoder_bilstm2_layer = model.get_layer('encoder_bilstm_2')\n",
    "# We need to rebuild the encoder to get proper outputs\n",
    "encoder_embedding_layer = model.get_layer('encoder_embedding')\n",
    "encoder_bilstm1_layer = model.get_layer('encoder_bilstm_1')\n",
    "\n",
    "# Rebuild encoder\n",
    "x = encoder_embedding_layer(encoder_inputs)\n",
    "x, fh1, fc1, bh1, bc1 = encoder_bilstm1_layer(x)\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_bilstm2_layer(x)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
    "print(\"Encoder model built for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda21ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build decoder model for inference\n",
    "decoder_inputs = model.input[1]\n",
    "decoder_state_input_h = Input(shape=(LSTM_UNITS * 2,))\n",
    "decoder_state_input_c = Input(shape=(LSTM_UNITS * 2,))\n",
    "encoder_outputs_input = Input(shape=(None, LSTM_UNITS * 2))\n",
    "\n",
    "# Get decoder layers\n",
    "decoder_embedding_layer = model.get_layer('decoder_embedding')\n",
    "decoder_lstm1_layer = model.get_layer('decoder_lstm_1')\n",
    "decoder_lstm2_layer = model.get_layer('decoder_lstm_2')\n",
    "attention_layer = model.get_layer('attention_layer')\n",
    "concat_layer = model.get_layer('concat_layer')\n",
    "decoder_dense_layer = model.get_layer('decoder_dense')\n",
    "\n",
    "# Build decoder inference path\n",
    "decoder_embedded = decoder_embedding_layer(decoder_inputs)\n",
    "decoder_out1, state_h1, state_c1 = decoder_lstm1_layer(\n",
    "    decoder_embedded, \n",
    "    initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
    ")\n",
    "decoder_out2, state_h2, state_c2 = decoder_lstm2_layer(decoder_out1)\n",
    "\n",
    "# Attention\n",
    "context = attention_layer([decoder_out2, encoder_outputs_input])\n",
    "decoder_combined = concat_layer([decoder_out2, context])\n",
    "decoder_outputs = decoder_dense_layer(decoder_combined)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs, encoder_outputs_input, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs, state_h2, state_c2]\n",
    ")\n",
    "print(\"Decoder model built for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(input_text, beam_width=BEAM_WIDTH):\n",
    "    \"\"\"\n",
    "    Perform beam search decoding for better quality outputs.\n",
    "    \n",
    "    Args:\n",
    "        input_text: English text to transliterate\n",
    "        beam_width: Number of beams to maintain\n",
    "        \n",
    "    Returns:\n",
    "        Best decoded Khmer text\n",
    "    \"\"\"\n",
    "    # Preprocess input\n",
    "    text = str(input_text).strip()\n",
    "    text = re.sub(r\"[^a-z]\", \"\", text.lower())\n",
    "    \n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Encode input\n",
    "    seq = eng_tokenizer.texts_to_sequences([text])\n",
    "    encoder_input = pad_sequences(seq, maxlen=max_eng_len, padding='post')\n",
    "    encoder_out, state_h, state_c = encoder_model.predict(encoder_input, verbose=0)\n",
    "    \n",
    "    # Start token\n",
    "    start_token = khm_tokenizer.word_index['\\t']\n",
    "    end_token = khm_tokenizer.word_index['\\n']\n",
    "    \n",
    "    # Initialize beams: (sequence, score, states)\n",
    "    beams = [([start_token], 0.0, state_h, state_c)]\n",
    "    \n",
    "    for _ in range(max_khm_len + 1):\n",
    "        all_candidates = []\n",
    "        \n",
    "        for seq, score, h, c in beams:\n",
    "            # If sequence ended, keep it as is\n",
    "            if seq[-1] == end_token:\n",
    "                all_candidates.append((seq, score, h, c))\n",
    "                continue\n",
    "            \n",
    "            # Get next token predictions\n",
    "            target_seq = np.array([[seq[-1]]])\n",
    "            predictions, new_h, new_c = decoder_model.predict(\n",
    "                [target_seq, encoder_out, h, c], \n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Get top k predictions\n",
    "            top_k_indices = np.argsort(predictions[0, 0, :])[-beam_width:]\n",
    "            \n",
    "            for idx in top_k_indices:\n",
    "                candidate_seq = seq + [idx]\n",
    "                # Use log probability to avoid underflow\n",
    "                candidate_score = score + np.log(predictions[0, 0, idx] + 1e-10)\n",
    "                all_candidates.append((candidate_seq, candidate_score, new_h, new_c))\n",
    "        \n",
    "        # Select top beams\n",
    "        ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
    "        beams = ordered[:beam_width]\n",
    "        \n",
    "        # Stop if all beams ended\n",
    "        if all(seq[-1] == end_token for seq, _, _, _ in beams):\n",
    "            break\n",
    "    \n",
    "    # Return best sequence\n",
    "    best_seq = beams[0][0]\n",
    "    decoded_chars = []\n",
    "    \n",
    "    for idx in best_seq[1:]:  # Skip start token\n",
    "        if idx == end_token:\n",
    "            break\n",
    "        char = khm_tokenizer.index_word.get(idx, '')\n",
    "        if char:\n",
    "            decoded_chars.append(char)\n",
    "    \n",
    "    return unicodedata.normalize('NFC', ''.join(decoded_chars))\n",
    "\n",
    "print(f\"Beam search decoder ready (beam width: {BEAM_WIDTH})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b731d35",
   "metadata": {},
   "source": [
    "## Comprehensive Evaluation Suite\n",
    "\n",
    "Implementing standard metrics for transliteration quality:\n",
    "- **BLEU Score**: Measures n-gram overlap with reference translations\n",
    "- **Character Error Rate (CER)**: Edit distance at character level\n",
    "- **Word Error Rate (WER)**: Accuracy at word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e601ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.metrics import edit_distance\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "def calculate_cer(reference, hypothesis):\n",
    "    \"\"\"Calculate Character Error Rate\"\"\"\n",
    "    if len(reference) == 0:\n",
    "        return 1.0 if len(hypothesis) > 0 else 0.0\n",
    "    \n",
    "    distance = edit_distance(reference, hypothesis)\n",
    "    return distance / len(reference)\n",
    "\n",
    "def calculate_wer(reference, hypothesis):\n",
    "    \"\"\"Calculate Word Error Rate\"\"\"\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "    \n",
    "    if len(ref_words) == 0:\n",
    "        return 1.0 if len(hyp_words) > 0 else 0.0\n",
    "    \n",
    "    distance = edit_distance(ref_words, hyp_words)\n",
    "    return distance / len(ref_words)\n",
    "\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    \"\"\"Calculate BLEU score with smoothing\"\"\"\n",
    "    reference_tokens = list(reference)\n",
    "    hypothesis_tokens = list(hypothesis)\n",
    "    \n",
    "    if len(hypothesis_tokens) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    smoothing = SmoothingFunction().method1\n",
    "    return sentence_bleu([reference_tokens], hypothesis_tokens, smoothing_function=smoothing)\n",
    "\n",
    "def evaluate_model(test_pairs, beam_width=BEAM_WIDTH, max_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluate model on test pairs with comprehensive metrics.\n",
    "    \n",
    "    Args:\n",
    "        test_pairs: List of (english, khmer) tuples\n",
    "        beam_width: Beam width for decoding\n",
    "        max_samples: Maximum number of samples to evaluate (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    if max_samples:\n",
    "        test_pairs = test_pairs[:max_samples]\n",
    "    \n",
    "    bleu_scores = []\n",
    "    cer_scores = []\n",
    "    wer_scores = []\n",
    "    exact_matches = 0\n",
    "    \n",
    "    print(f\"Evaluating on {len(test_pairs)} samples with beam width {beam_width}...\")\n",
    "    \n",
    "    for i, (eng, khm_ref) in enumerate(test_pairs):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(test_pairs)} samples...\")\n",
    "        \n",
    "        # Generate prediction\n",
    "        khm_pred = beam_search_decode(eng, beam_width=beam_width)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        bleu = calculate_bleu(khm_ref, khm_pred)\n",
    "        cer = calculate_cer(khm_ref, khm_pred)\n",
    "        wer = calculate_wer(khm_ref, khm_pred)\n",
    "        \n",
    "        bleu_scores.append(bleu)\n",
    "        cer_scores.append(cer)\n",
    "        wer_scores.append(wer)\n",
    "        \n",
    "        if khm_pred == khm_ref:\n",
    "            exact_matches += 1\n",
    "    \n",
    "    results = {\n",
    "        'bleu': {\n",
    "            'mean': np.mean(bleu_scores),\n",
    "            'std': np.std(bleu_scores),\n",
    "            'median': np.median(bleu_scores)\n",
    "        },\n",
    "        'cer': {\n",
    "            'mean': np.mean(cer_scores),\n",
    "            'std': np.std(cer_scores),\n",
    "            'median': np.median(cer_scores)\n",
    "        },\n",
    "        'wer': {\n",
    "            'mean': np.mean(wer_scores),\n",
    "            'std': np.std(wer_scores),\n",
    "            'median': np.median(wer_scores)\n",
    "        },\n",
    "        'exact_match_rate': exact_matches / len(test_pairs),\n",
    "        'num_samples': len(test_pairs)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Evaluation functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d767bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test set from the original dataset (use a held-out portion)\n",
    "np.random.seed(42)\n",
    "test_size = min(500, len(dataset) // 5)  # 20% or max 500 samples\n",
    "test_indices = np.random.choice(len(dataset), size=test_size, replace=False)\n",
    "test_pairs = [dataset[i] for i in test_indices]\n",
    "\n",
    "print(f\"Test set size: {len(test_pairs)} samples\")\n",
    "print(\"\\nSample test pairs:\")\n",
    "for i in range(5):\n",
    "    print(f\"  {test_pairs[i][0]} -> {test_pairs[i][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation\n",
    "eval_results = evaluate_model(test_pairs, beam_width=BEAM_WIDTH)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTest Set Size: {eval_results['num_samples']} samples\")\n",
    "print(f\"Beam Width: {BEAM_WIDTH}\")\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"  BLEU Score:\")\n",
    "print(f\"    Mean:   {eval_results['bleu']['mean']:.4f} ± {eval_results['bleu']['std']:.4f}\")\n",
    "print(f\"    Median: {eval_results['bleu']['median']:.4f}\")\n",
    "print(f\"\\n  Character Error Rate (CER):\")\n",
    "print(f\"    Mean:   {eval_results['cer']['mean']:.4f} ± {eval_results['cer']['std']:.4f}\")\n",
    "print(f\"    Median: {eval_results['cer']['median']:.4f}\")\n",
    "print(f\"\\n  Word Error Rate (WER):\")\n",
    "print(f\"    Mean:   {eval_results['wer']['mean']:.4f} ± {eval_results['wer']['std']:.4f}\")\n",
    "print(f\"    Median: {eval_results['wer']['median']:.4f}\")\n",
    "print(f\"\\n  Exact Match Rate: {eval_results['exact_match_rate']:.2%}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e87a0b6",
   "metadata": {},
   "source": [
    "## Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f216282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for best fold\n",
    "best_history = fold_histories[best_fold_idx]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss plot\n",
    "axes[0, 0].plot(best_history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0, 0].plot(best_history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].set_title(f'Training History - Best Fold ({best_fold_idx + 1})', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0, 1].plot(best_history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(best_history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0, 1].set_title('Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Cross-validation scores\n",
    "fold_numbers = list(range(1, K_FOLDS + 1))\n",
    "fold_losses = [score['loss'] for score in fold_scores]\n",
    "fold_accs = [score['accuracy'] for score in fold_scores]\n",
    "\n",
    "axes[1, 0].bar(fold_numbers, fold_losses, color='steelblue', alpha=0.7)\n",
    "axes[1, 0].axhline(y=avg_val_loss, color='red', linestyle='--', label=f'Mean: {avg_val_loss:.4f}')\n",
    "axes[1, 0].set_title('Validation Loss per Fold', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Fold')\n",
    "axes[1, 0].set_ylabel('Validation Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[1, 1].bar(fold_numbers, fold_accs, color='forestgreen', alpha=0.7)\n",
    "axes[1, 1].axhline(y=avg_val_accuracy, color='red', linestyle='--', label=f'Mean: {avg_val_accuracy:.4f}')\n",
    "axes[1, 1].set_title('Validation Accuracy per Fold', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Fold')\n",
    "axes[1, 1].set_ylabel('Validation Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa94b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evaluation metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics = ['BLEU', 'CER', 'WER']\n",
    "metric_keys = ['bleu', 'cer', 'wer']\n",
    "colors = ['steelblue', 'coral', 'mediumseagreen']\n",
    "\n",
    "for idx, (metric, key, color) in enumerate(zip(metrics, metric_keys, colors)):\n",
    "    mean_val = eval_results[key]['mean']\n",
    "    std_val = eval_results[key]['std']\n",
    "    median_val = eval_results[key]['median']\n",
    "    \n",
    "    axes[idx].bar(['Mean', 'Median'], [mean_val, median_val], color=color, alpha=0.7, width=0.5)\n",
    "    axes[idx].errorbar(['Mean'], [mean_val], yerr=[std_val], fmt='none', color='black', capsize=10, capthick=2)\n",
    "    axes[idx].set_title(f'{metric} Score', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Score')\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate([mean_val, median_val]):\n",
    "        axes[idx].text(i, v + 0.02, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nExact Match Rate: {eval_results['exact_match_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92914ee6",
   "metadata": {},
   "source": [
    "## Example Predictions\n",
    "\n",
    "Let's test the improved model with beam search on some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03ac9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test examples\n",
    "test_examples = [\n",
    "    \"hello\",\n",
    "    \"trap\",\n",
    "    \"mean luy\",\n",
    "    \"kdas\",\n",
    "    \"cambodia\",\n",
    "    \"phnom penh\",\n",
    "    \"angkor wat\",\n",
    "    \"khmer\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXAMPLE PREDICTIONS WITH BEAM SEARCH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for example in test_examples:\n",
    "    result = beam_search_decode(example, beam_width=BEAM_WIDTH)\n",
    "    print(f\"\\nInput:  {example}\")\n",
    "    print(f\"Output: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example predictions with references from test set\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON WITH REFERENCE TRANSLATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "np.random.seed(123)\n",
    "sample_indices = np.random.choice(len(test_pairs), size=min(10, len(test_pairs)), replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    eng, khm_ref = test_pairs[idx]\n",
    "    khm_pred = beam_search_decode(eng, beam_width=BEAM_WIDTH)\n",
    "    \n",
    "    # Calculate metrics for this example\n",
    "    bleu = calculate_bleu(khm_ref, khm_pred)\n",
    "    cer = calculate_cer(khm_ref, khm_pred)\n",
    "    \n",
    "    print(f\"\\nInput:      {eng}\")\n",
    "    print(f\"Reference:  {khm_ref}\")\n",
    "    print(f\"Prediction: {khm_pred}\")\n",
    "    print(f\"BLEU: {bleu:.4f} | CER: {cer:.4f} | Match: {'✓' if khm_ref == khm_pred else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0613de",
   "metadata": {},
   "source": [
    "## Summary of Improvements\n",
    "\n",
    "This improved model implements:\n",
    "\n",
    "1. **Attention Mechanism**: The attention layer allows the decoder to dynamically focus on relevant parts of the input sequence, significantly improving translation quality for longer sequences.\n",
    "\n",
    "2. **Bidirectional Encoder**: Using bidirectional LSTMs in the encoder captures context from both past and future, providing richer representations.\n",
    "\n",
    "3. **Deeper Networks**: 2-layer LSTMs in both encoder and decoder increase model capacity to learn complex patterns.\n",
    "\n",
    "4. **Beam Search Decoding**: Exploring top-3 candidate sequences during inference produces higher quality outputs compared to greedy decoding.\n",
    "\n",
    "5. **Data Augmentation**: Character-level augmentation increases training data diversity and model robustness.\n",
    "\n",
    "6. **K-Fold Cross-Validation**: 5-fold cross-validation provides more reliable performance estimates and helps prevent overfitting.\n",
    "\n",
    "7. **Comprehensive Evaluation**: Standard metrics (BLEU, CER, WER) allow proper comparison with other transliteration systems.\n",
    "\n",
    "8. **Increased Model Capacity**: Larger embedding dimensions (64 vs 32) and LSTM units (128 vs 64) allow the model to learn more complex mappings.\n",
    "\n",
    "### Key Benefits:\n",
    "- More robust training with cross-validation\n",
    "- Better sequence modeling with attention and bidirectional processing\n",
    "- Higher quality predictions with beam search\n",
    "- Comprehensive evaluation for proper performance assessment\n",
    "- Increased model capacity for complex patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
